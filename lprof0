Timer unit: 1e-07 s

Total time: 4.88117 s
File: G:/Repos\bs.py
Function: bs at line 8

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     8                                           def bs():
     9         1         54.0     54.0      0.0      pp = "G:\\Repos\\trikit\\trikit\\datasets\\RAA.csv"
    10         1      28337.0  28337.0      0.1      df = pd.read_csv(pp)
    11         1     240132.0 240132.0      0.5      tri = trikit.triangle.CumTriangle(data=df, origin="origin", dev="dev", value="value")
    12         1    7195604.0 7195604.0     14.7      ldfs = tri.cl().ldfs
    13         1       1246.0   1246.0      0.0      fitted_tri_cum = tri.copy(deep=True)
    14                                           
    15        11        184.0     16.7      0.0      for ii in range(fitted_tri_cum.shape[0]):
    16                                           
    17        10      23110.0   2311.0      0.0          iterrow = fitted_tri_cum.iloc[ii, :]
    18        10      25790.0   2579.0      0.1          if iterrow.isnull().any():
    19                                                       # Find first NaN element in iterrow.
    20         9      86232.0   9581.3      0.2              nan_hdr = iterrow.isnull()[iterrow.isnull()==True].index[0]
    21         9        944.0    104.9      0.0              nan_idx = fitted_tri_cum.columns.tolist().index(nan_hdr)
    22         9        137.0     15.2      0.0              init_idx = nan_idx - 1
    23                                                   else:
    24                                                       # If here, iterrow is the most mature exposure period.
    25         1         41.0     41.0      0.0              init_idx = fitted_tri_cum.shape[1] - 1
    26                                           
    27                                                   # Set to NaN any development periods earlier than init_idx.
    28        10      25530.0   2553.0      0.1          fitted_tri_cum.iloc[ii, :init_idx] = np.NaN
    29                                           
    30                                                   # Iterate over rows, undeveloping triangle from latest diagonal.
    31        55      38437.0    698.9      0.1          for j in range(fitted_tri_cum.iloc[ii, :init_idx].size, 0, -1):
    32        45        747.0     16.6      0.0              prev_col_idx, curr_col_idx, curr_ldf_idx = j, j - 1, j - 1
    33        45      34287.0    761.9      0.1              prev_col_val = fitted_tri_cum.iloc[ii, prev_col_idx]
    34        45       8664.0    192.5      0.0              curr_ldf_val = ldfs.iloc[curr_ldf_idx]
    35        45     104770.0   2328.2      0.2              fitted_tri_cum.iloc[ii, curr_col_idx] = (prev_col_val / curr_ldf_val)
    36                                           
    37                                           
    38                                           
    39         1       2084.0   2084.0      0.0      fitted_tri_incr = fitted_tri_cum.diff(axis=1)
    40         1       3102.0   3102.0      0.0      fitted_tri_incr.iloc[:, 0] = fitted_tri_cum.iloc[:, 0]
    41                                           
    42         1     280756.0 280756.0      0.6      I = tri.to_incr()
    43         1         25.0     25.0      0.0      m = fitted_tri_incr
    44         1      35698.0  35698.0      0.1      resid_us = (I - m) / np.sqrt(m.abs())
    45                                           
    46         1      11649.0  11649.0      0.0      dof_ = tri.nbr_cells - (tri.columns.size - 1) + tri.index.size
    47         1       4272.0   4272.0      0.0      resid_adj = np.sqrt(tri.nbr_cells / dof_) * resid_us
    48                                           
    49         1       5504.0   5504.0      0.0      resid_ = resid_adj.iloc[:-1,:-1].values.ravel()
    50         1        155.0    155.0      0.0      sampling_dist = resid_[np.logical_and(~np.isnan(resid_), resid_!=0)]
    51                                           
    52                                           
    53                                           
    54                                               # _bs_samples
    55                                           
    56         1         14.0     14.0      0.0      random_state = 516
    57         1         12.0     12.0      0.0      neg_handler="first"
    58         1         12.0     12.0      0.0      sims = 1000
    59         1         13.0     13.0      0.0      parametric = False
    60                                           
    61         1         75.0     75.0      0.0      from numpy.random import RandomState
    62         1         13.0     13.0      0.0      if random_state is not None:
    63         1         15.0     15.0      0.0          if isinstance(random_state, int):
    64         1       2189.0   2189.0      0.0              prng = RandomState(random_state)
    65                                                   elif isinstance(random_state, RandomState):
    66                                                       prng = random_state
    67                                               else:
    68                                                   prng = RandomState()
    69                                           
    70         1         35.0     35.0      0.0      sampling_dist = sampling_dist.flatten()
    71                                           
    72         1      19958.0  19958.0      0.0      fti = fitted_tri_incr.reset_index(drop=False).rename({"index":"origin"}, axis=1)
    73         1      37704.0  37704.0      0.1      dfm = pd.melt(fti, id_vars=["origin"], var_name="dev", value_name="value")
    74         2      26177.0  13088.5      0.1      dfm = dfm[~np.isnan(dfm["value"])].astype(
    75         1         26.0     26.0      0.0          {"origin":np.int_, "dev":np.int_, "value":np.float_}
    76                                                   )
    77                                           
    78                                               # Handle first period negative cells as specified by `neg_handler`.
    79         1       3431.0   3431.0      0.0      if np.any(dfm["value"]<0):
    80                                           
    81                                                   if neg_handler=="first":
    82                                                       dfm["value"] = np.where(
    83                                                           np.logical_and(dfm["dev"].values==1, dfm["value"].values<0),
    84                                                           1., dfm["value"].values
    85                                                       )
    86                                           
    87                                                   elif neg_handler=="all":
    88                                                       # Obtain reference to minimum triangle cell value, then
    89                                                       # add the absolute value of that amount plus one to all
    90                                                       # other triangle cells.
    91                                                       add2cells = np.abs(dfm["value"].min()) + 1
    92                                                       dfm["value"] = dfm["value"] + add2cells
    93                                           
    94                                                   else:
    95                                                       raise ValueError("`neg_handler` must be in ['first', 'all'].")
    96                                           
    97                                           
    98         1     145820.0 145820.0      0.3      dfi = tri.to_tbl(drop_nas=False).drop("value", axis=1)
    99         1      41596.0  41596.0      0.1      dfp = dfi.merge(dfm, how="outer", on=["origin", "dev"])
   100         1       7761.0   7761.0      0.0      dfp["rectype"] = np.where(np.isnan(dfp["value"].values), "forecast", "actual")
   101         1      10361.0  10361.0      0.0      dfp = dfp.rename({"value":"incr"}, axis=1)
   102         1      11396.0  11396.0      0.0      dfp["incr_sqrt"] = np.sqrt(dfp["incr"].values)
   103         2         52.0     26.0      0.0      dfrtypes = {"origin":np.int, "dev":np.int_, "incr":np.float_,
   104         1         15.0     15.0      0.0                  "incr_sqrt":np.float_, "rectype":np.str,}
   105         1         15.0     15.0      0.0      dfrcols = ["origin", "dev", "incr", "rectype", "incr_sqrt"]
   106         1     288827.0 288827.0      0.6      dfr = pd.DataFrame(np.tile(dfp, (sims, 1)), columns=dfrcols).astype(dfrtypes)
   107         1      22499.0  22499.0      0.0      dfr["sim"] = np.divmod(dfr.index, tri.shape[0] * tri.shape[1])[0]
   108         1         50.0     50.0      0.0      sample_size = dfr.shape[0]
   109                                           
   110         1         14.0     14.0      0.0      if parametric:
   111                                                   # Sample random residuals from normal distribution with zero mean.
   112                                                   dfr["resid"] = prng.normal(
   113                                                       loc=0, scale=sampling_dist.std(ddof=1), size=sample_size
   114                                                       )
   115                                               else:
   116                                                   # Sample random residual from adjusted pearson residuals.
   117         2      17936.0   8968.0      0.0          dfr["resid"] = prng.choice(
   118         1         14.0     14.0      0.0              sampling_dist, sample_size, replace=True
   119                                                       )
   120                                           
   121                                               # Calcuate resampled incremental and cumulative losses.
   122         1      17104.0  17104.0      0.0      dfr["resid"] = np.where(dfr["rectype"].values=="forecast", np.NaN, dfr["resid"].values)
   123         1     261825.0 261825.0      0.5      dfr = dfr.sort_values(by=["sim", "origin", "dev"]).reset_index(drop=True)
   124         1      20548.0  20548.0      0.0      dfr["samp_incr"] = dfr["incr"].values + dfr["resid"].values * dfr["incr_sqrt"].values
   125         1     155329.0 155329.0      0.3      dfr["samp_cum"]  = dfr.groupby(["sim", "origin"], as_index=False)["samp_incr"].cumsum()
   126                                               # dfsamples = dfr.reset_index(drop=True)
   127                                           
   128                                               # samp_path = "G:/Repos/Temp/dfsamples.pkl"
   129                                               # with open(samp_path, 'wb') as f:
   130                                               #     pickle.dump(dfsamples, f, pickle.HIGHEST_PROTOCOL)
   131                                           
   132                                               # dfsamples
   133                                               # samp_path = "G:/Repos/Temp/dfsamples.pkl"
   134                                               # with open(samp_path, 'wb') as f:
   135                                               #     pickle.dump(dfsamples, f, pickle.HIGHEST_PROTOCOL)
   136         1         26.0     26.0      0.0      samp_path = "G:/Repos/Temp/dfsamples.pkl"
   137         1        941.0    941.0      0.0      with open(samp_path, "rb") as fpkl:
   138         1      66373.0  66373.0      0.1          dfsamples = pickle.load(fpkl)
   139                                           
   140                                               # _bs_ldfs.
   141         1         25.0     25.0      0.0      ldfs_path = "G:/Repos/Temp/dfldfs1.pkl"
   142         1        722.0    722.0      0.0      with open(ldfs_path, "rb") as fpkl:
   143         1      13804.0  13804.0      0.0          bs_ldfs = pickle.load(fpkl)#.rename({"ldf":"ldf0"}, axis=1)
   144                                           
   145                                               # bs_forecasts ----------------------------------------------------------------
   146                                           
   147         1      48894.0  48894.0      0.1      dflvi = tri.rlvi.reset_index(drop=False)
   148         1      14872.0  14872.0      0.0      dflvi = dflvi.rename({"index":"origin", "dev":"l_act_dev"}, axis=1)
   149         1      13474.0  13474.0      0.0      dflvi = dflvi.drop("col_offset", axis=1)
   150         1       9184.0   9184.0      0.0      scale_param = (resid_us**2).sum().sum() / dof_
   151         1     279893.0 279893.0      0.6      dfcombined = dfsamples.merge(bs_ldfs, on=["sim", "dev"], how="left")
   152         1     186183.0 186183.0      0.4      dfcombined = dfcombined.merge(dflvi, on=["origin"], how="left")
   153         1     310518.0 310518.0      0.6      dfcombined = dfcombined.reset_index(drop=True).sort_values(by=["sim", "origin", "dev"])
   154                                           
   155                                           
   156         1       3726.0   3726.0      0.0      min_origin_year = dfcombined["origin"].values.min()
   157         2       9701.0   4850.5      0.0      dfcombined["_l_init_indx"] = np.where(
   158         1       3359.0   3359.0      0.0          dfcombined["dev"].values>=dfcombined["l_act_dev"].values, dfcombined.index.values, -1)
   159         1      63723.0  63723.0      0.1      dfacts = dfcombined[(dfcombined["origin"].values==min_origin_year) | (dfcombined["_l_init_indx"].values==-1)]
   160         1     256933.0 256933.0      0.5      dffcst = dfcombined[~dfcombined.index.isin(dfacts.index)].sort_values(by=["sim", "origin", "dev"])
   161         1      94208.0  94208.0      0.2      dffcst["_l_act_indx"] = dffcst.groupby(["sim", "origin"])["_l_init_indx"].transform("min")
   162         1     371251.0 371251.0      0.8      dffcst["l_act_cum"] = dffcst.lookup(dffcst["_l_act_indx"].values, ["samp_cum"] * dffcst.shape[0])
   163         1     115234.0 115234.0      0.2      dffcst["_cum_ldf"] = dffcst.groupby(["sim", "origin"])["ldf"].transform("cumprod").shift(periods=1)
   164         1      17518.0  17518.0      0.0      dffcst["_samp_cum2"] = dffcst["l_act_cum"].values * dffcst["_cum_ldf"].values
   165         1       4805.0   4805.0      0.0      dffcst["_samp_cum2"] = np.where(np.isnan(dffcst["_samp_cum2"].values), 0, dffcst["_samp_cum2"].values)
   166         1       8780.0   8780.0      0.0      dffcst["cum_final"] = np.where(np.isnan(dffcst["samp_cum"].values), 0, dffcst["samp_cum"].values) + dffcst["_samp_cum2"].values
   167                                           
   168                                               # Combine forecasts with actuals then compute incremental losses by sim and origin.
   169         1     167779.0 167779.0      0.3      dffcst = dffcst.drop(labels=["samp_cum", "samp_incr"], axis=1).rename(columns={"cum_final":"samp_cum"})
   170         1     531837.0 531837.0      1.1      dfsqrd = pd.concat([dffcst, dfacts], sort=True).sort_values(by=["sim", "origin", "dev"])
   171         1      19191.0  19191.0      0.0      dfsqrd["_dev1_ind"] = (dfsqrd["dev"].values==1) * 1
   172         1      12975.0  12975.0      0.0      dfsqrd["_incr_dev1"] = dfsqrd["_dev1_ind"].values * dfsqrd["samp_cum"].values
   173         1   35260111.0 35260111.0     72.2      dfsqrd["_incr_dev2"] = dfsqrd.groupby(["sim", "origin"])["samp_cum"].diff(periods=1)
   174         1       7331.0   7331.0      0.0      dfsqrd["_incr_dev2"] = np.where(np.isnan(dfsqrd["_incr_dev2"].values), 0, dfsqrd["_incr_dev2"].values)
   175         1       5156.0   5156.0      0.0      dfsqrd["samp_incr"] = dfsqrd["_incr_dev1"].values + dfsqrd["_incr_dev2"].values
   176         1      12278.0  12278.0      0.0      dfsqrd["var"] = np.abs(dfsqrd["samp_incr"].values * scale_param)
   177         1      11812.0  11812.0      0.0      dfsqrd["sign"] = np.where(dfsqrd["samp_incr"].values > 0, 1, -1)
   178         1     174461.0 174461.0      0.4      dfsqrd = dfsqrd.drop(labels=[i for i in dfsqrd.columns if i.startswith("_")], axis=1)
   179                                           
   180         1     243028.0 243028.0      0.5      dfsqrd = dfsqrd.sort_values(by=["sim", "origin", "dev"]).reset_index(drop=True)
   181                                           
   182         1     232666.0 232666.0      0.5      dfforecasts = dfsqrd.sort_values(by=["sim", "origin", "dev"]).reset_index(drop=True)
   183         1      10075.0  10075.0      0.0      dfforecasts["param2"] = scale_param
   184         1      12225.0  12225.0      0.0      dfforecasts["param1"] = np.abs(dfforecasts["samp_incr"].values / dfforecasts["param2"].values)
   185         1         22.0     22.0      0.0      def fdist(param1, param2):
   186                                                   """gamma.rvs(a=param1, scale=param2, size=1, random_state=None)"""
   187                                                   return(prng.gamma(param1, param2))
   188                                           
   189         2       8540.0   4270.0      0.0      dfforecasts["final_incr"] = np.where(
   190         1      11813.0  11813.0      0.0          dfforecasts["rectype"].values=="forecast",
   191         1      65149.0  65149.0      0.1          fdist(dfforecasts["param1"].values, dfforecasts["param2"].values) * dfforecasts["sign"].values,
   192         1        701.0    701.0      0.0          dfforecasts["samp_incr"].values
   193                                                   )
   194         1     185377.0 185377.0      0.4      dfforecasts["final_cum"] = dfforecasts.groupby(["sim", "origin"])["final_incr"].cumsum()
   195         1     197241.0 197241.0      0.4      dfforecasts = dfforecasts.rename({"final_cum":"ultimate", "l_act_cum":"latest"}, axis=1)
   196         1     249705.0 249705.0      0.5      dfprocerror = dfforecasts.sort_values(by=["sim", "origin", "dev"]).reset_index(drop=True)
   197                                           
   198         1         31.0     31.0      0.0      keepcols = ["sim", "origin", "latest", "ultimate", "reserve"]
   199         1       2245.0   2245.0      0.0      max_devp = dfprocerror["dev"].values.max()
   200         1      23569.0  23569.0      0.0      dfprocerror["reserve"] = dfprocerror["ultimate"] - dfprocerror["latest"]
   201         1     174842.0 174842.0      0.4      dfreserves = dfprocerror[dfprocerror["dev"].values==max_devp][keepcols].drop_duplicates()
   202         2       3655.0   1827.5      0.0      dfreserves["latest"]  = np.where(
   203         1       1872.0   1872.0      0.0          np.isnan(dfreserves["latest"].values),
   204         1        998.0    998.0      0.0          dfreserves["ultimate"].values, dfreserves["latest"].values
   205                                                   )
   206         1       6219.0   6219.0      0.0      dfreserves["reserve"] = np.nan_to_num(dfreserves["reserve"].values, 0)
   207         1      36333.0  36333.0      0.1      dfreserves = dfreserves.sort_values(by=["origin", "sim"]).reset_index(drop=True)
   208         1         19.0     19.0      0.0      return(dfreserves)